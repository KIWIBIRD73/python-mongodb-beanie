import asyncio
from datetime import datetime
from typing import Literal
from src.db import init_db
from src.models import NLPResult, Preprocessing, Entity, PosTag
from src.nlp import (
    tokenize,
    to_lower_text,
    remove_punctuation,
    extract_lemmas,
    extract_entities,
    extract_pos
)

def get_input_sentence() -> str:
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–≤–æ–¥–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è NLP –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑ —Ç–µ—Ä–º–∏–Ω–∞–ª–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    
    :return: –í–≤–µ–¥–µ–Ω–Ω–æ–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
    :rtype: str
    """
    sentence = input('–í–≤–µ–¥–∏—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è NLP –∞–Ω–∞–ª–∏–∑–∞: ')
    if not sentence.strip():
        raise ValueError('–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –Ω–µ –±—ã–ª–æ –≤–≤–µ–¥–µ–Ω–æ')
    return sentence.strip()


def get_language() -> Literal['ru', 'en']:
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–≤–æ–¥–∞ —è–∑—ã–∫–∞ –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –±—É–¥–µ—Ç –ø—Ä–æ–≤–æ–¥–∏—Ç—å—Å—è NLP –æ–±—Ä–∞–±–æ—Ç–∫–∞
    
    :return: –õ–æ–∫–∞–ª—å –¥–ª—è –≤–≤–µ–¥–µ–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è `ru` –∏–ª–∏ `en`
    :rtype: Literal['ru', 'en']
    """
    lang = input("–í–≤–µ–¥–∏—Ç–µ —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞ ('ru' –∏–ª–∏ 'en'): ").strip().lower()

    # —É–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –ø–µ—Ä–µ–¥–∞–Ω–∞ –æ–¥–Ω–æ –∏–∑ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –ª–æ–∫–∞–ª–µ–π
    available_locales = ('ru', 'en')
    while lang not in available_locales:
        lang = input("–í–≤–µ–¥–∏—Ç–µ –æ–¥–∏–Ω –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤ ('ru' –∏–ª–∏ 'en'): ").strip().lower()

    return lang


async def analyze_and_save():
    """
    –ó–∞–ø—É—Å–∫ NLP –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏–∑ `get_input_sentence` –∏ –ª–æ–∫–∞–ª–∏ –∏–∑ `get_language`
    """
    sentence = get_input_sentence()
    language = get_language()

    print('üöÄ –ó–∞–ø—É—Å–∫ NLP –∞–Ω–∞–ª–∏–∑–∞...')

    # –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
    print('‚è≥ –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥...')
    tokens_word = tokenize(sentence, 'word')
    tokens_symbol = tokenize(sentence, 'symbol')
    lowercased = to_lower_text(sentence)
    no_punct = remove_punctuation(sentence, 'no_punct')
    alnum_keep = remove_punctuation(sentence, 'alnum_keep')

    preprocessing = Preprocessing(
        tokens_by_word=tokens_word,
        tokens_by_symbol=tokens_symbol,
        lowercased=lowercased,
        no_punct=no_punct,
        alnum_keep=alnum_keep
    )

    # NLP –∞–Ω–∞–ª–∏–∑
    print('‚è≥ NLP –∞–Ω–∞–ª–∏–∑...')
    lemmas = extract_lemmas(sentence, language)
    entities_list = [Entity(text=text, label=label) for text, label in extract_entities(sentence, language)]
    pos_tags_list = [PosTag(text=text, pos=pos) for text, pos in extract_pos(sentence, language)]

    # –°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ mongoDB –≤ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é nlp_result
    doc = NLPResult(
        raw_text=sentence,
        language=language,
        preprocessing=preprocessing,
        lemmas=lemmas,
        entities=entities_list,
        pos_tags=pos_tags_list,
        created_at=datetime.utcnow()
    )

    print('‚è≥ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö...')
    # –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–æ–∑–¥–∞–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
    await doc.insert()
    print("‚úÖ NLP –∞–Ω–∞–ª–∏–∑ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö!")
    print(f"üíæ ID —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {doc.id}")


async def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –∑–∞–ø—É—Å–∫–∞–µ—Ç –ø—Ä–æ–µ–∫—Ç
    """
    await init_db()
    await analyze_and_save()

# –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
# –î–ª—è –∑–∞–ø—É—Å–∫–∞ —á–µ—Ä–µ–∑ Poetry
def run():
    asyncio.run(main())

# –î–ª—è –∑–∞–ø—É—Å–∫–∞ —á–µ—Ä–µ–∑ Python
if __name__ == "__main__":
    run()